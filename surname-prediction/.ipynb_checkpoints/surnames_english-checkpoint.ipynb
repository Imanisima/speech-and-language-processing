{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Sequence Modeling - Russian vs English Surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Goal\n",
    "---\n",
    "\n",
    "Develop and classifier for Russian vs English surnames.\n",
    "\n",
    "In this iteration we are going to:\n",
    "* Compute bigram frequencies for English names.\n",
    "* Compute bigram frequencies for Russian names.\n",
    "* Develop a bag of bigrams model for distinguishing English and Russian names.\n",
    "* Implement Katz's Back-Off Model Smoothing\n",
    "* Test performance of model using English data.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk import bigrams, trigrams, word_tokenize\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import brown # corpus of english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB # Multi Naive Bayes with discrete values\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # tokenize texts/build vocab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer # tokenizes text and normalizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's perform some EDA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file into data frame.\n",
    "surname_csv = \"data_set/russian_and_english_dev.csv\"\n",
    "surname_df = pd.read_csv(surname_csv, index_col = None, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename dev data columns.\n",
    "surname_df.rename(columns = {'Unnamed: 0':'surname', 'Unnamed: 1':'nationality'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Mokrousov\n",
       "1              Nurov\n",
       "2           Judovich\n",
       "3       Mikhailjants\n",
       "4         Jandarbiev\n",
       "            ...     \n",
       "1301          Foxall\n",
       "1302           Cowan\n",
       "1303       Wrightson\n",
       "1304            Loft\n",
       "1305            Bird\n",
       "Name: surname, Length: 1306, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing non-alphabetic characters \n",
    "surname_names = surname_df['surname'].apply(lambda x: re.sub('[^a-zA-Z]', '', x))\n",
    "surname_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_names.to_csv(\"data_set/corpus/surnames_names.txt\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>Fairhurst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>Wateridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>Nemeth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>Moroney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>Goodall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       surname\n",
       "940  Fairhurst\n",
       "941  Wateridge\n",
       "942     Nemeth\n",
       "943    Moroney\n",
       "944    Goodall"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve English names only\n",
    "english_df = surname_df.loc[surname_df[\"nationality\"] == \"English\"]\n",
    "english_df = english_df[[\"surname\"]]\n",
    "english_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all english names in txt file\n",
    "english_df.to_csv(\"data_set/corpus/english_names.txt\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Mokrousov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Nurov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Judovich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Mikhailjants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Jandarbiev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        surname\n",
       "0     Mokrousov\n",
       "1         Nurov\n",
       "2      Judovich\n",
       "3  Mikhailjants\n",
       "4    Jandarbiev"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Russian names only\n",
    "russian_df = surname_df.loc[surname_df[\"nationality\"] == \"Russian\"]\n",
    "russian_df = russian_df[[\"surname\"]]\n",
    "russian_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_df.to_csv(\"data_set/corpus/russian_names.txt\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create New Corpus\n",
    "\n",
    "---\n",
    "Create a new corpus of English and Russian names to be used for the n_gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all names\n",
    "# English\n",
    "names = open(\"data_set/corpus/surnames_names.txt\", \"r\")\n",
    "surname_names = [x.rstrip() for x in names.readlines()]\n",
    "surname_names = [x.lower() for x in surname_df]\n",
    "# surname_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "names = open(\"data_set/corpus/english_names.txt\", \"r\")\n",
    "english_names = [x.rstrip() for x in names.readlines()]\n",
    "english_names = [x.lower() for x in english_names]\n",
    "# english_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian\n",
    "names = open(\"data_set/corpus/russian_names.txt\", \"r\")\n",
    "russian_names = [x.rstrip() for x in names.readlines()]\n",
    "russian_names = [x.lower() for x in russian_names]\n",
    "# russian_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Calculate Frequencies and Probabilities\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate bigrams and frequencies\n",
    "def generate_bigrams(names):\n",
    "    n_gram = collections.Counter()\n",
    "    for c in names:\n",
    "        n_gram.update(Counter(c[idx : idx + 2] for idx in range(len(c) - 1)))\n",
    "        \n",
    "    return n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting frequences in descending order\n",
    "def freq_sorted(n_gram):\n",
    "    [print(key, value) for (key, value) in sorted(n_gram.items(), key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve english bigrams\n",
    "eng_gram = generate_bigrams(english_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er 49\n",
      "on 47\n",
      "ar 40\n",
      "in 37\n",
      "le 37\n",
      "an 36\n",
      "ey 33\n",
      "ll 33\n",
      "ne 28\n",
      "en 26\n",
      "el 25\n",
      "ra 25\n",
      "th 23\n",
      "or 23\n",
      "re 23\n",
      "to 23\n",
      "ck 21\n",
      "ma 20\n",
      "st 19\n",
      "ri 19\n",
      "de 19\n",
      "ur 18\n",
      "es 18\n",
      "ha 17\n",
      "so 17\n",
      "la 17\n",
      "rt 16\n",
      "ke 16\n",
      "ro 15\n",
      "ho 15\n",
      "ou 15\n",
      "is 15\n",
      "al 14\n",
      "am 14\n",
      "ng 14\n",
      "ns 14\n",
      "pe 14\n",
      "nd 14\n",
      "se 14\n",
      "od 13\n",
      "ld 13\n",
      "il 13\n",
      "nn 13\n",
      "ol 13\n",
      "mo 12\n",
      "as 12\n",
      "sh 12\n",
      "tt 12\n",
      "ic 12\n",
      "wa 11\n",
      "te 11\n",
      "be 11\n",
      "rd 11\n",
      "co 11\n",
      "rs 10\n",
      "at 10\n",
      "ge 10\n",
      "ga 10\n",
      "he 10\n",
      "do 10\n",
      "ea 10\n",
      "hi 10\n",
      "rr 10\n",
      "lo 10\n",
      "oo 9\n",
      "wo 9\n",
      "ul 9\n",
      "li 9\n",
      "ki 9\n",
      "ow 9\n",
      "ir 8\n",
      "et 8\n",
      "rn 8\n",
      "gh 8\n",
      "pa 8\n",
      "ve 8\n",
      "wi 8\n",
      "we 8\n",
      "no 7\n",
      "ak 7\n",
      "tr 7\n",
      "ed 7\n",
      "rl 7\n",
      "dd 7\n",
      "vi 7\n",
      "ch 7\n",
      "oc 7\n",
      "un 7\n",
      "ei 7\n",
      "ss 7\n",
      "ad 7\n",
      "di 7\n",
      "ie 7\n",
      "hu 6\n",
      "em 6\n",
      "da 6\n",
      "ag 6\n",
      "na 6\n",
      "ut 6\n",
      "gr 6\n",
      "ee 6\n",
      "dr 6\n",
      "bi 6\n",
      "ta 6\n",
      "nt 6\n",
      "ds 6\n",
      "fo 6\n",
      "um 6\n",
      "fi 6\n",
      "ug 5\n",
      "gg 5\n",
      "it 5\n",
      "ks 5\n",
      "ay 5\n",
      "au 5\n",
      "bb 5\n",
      "ru 5\n",
      "ni 5\n",
      "ig 5\n",
      "aw 5\n",
      "dl 5\n",
      "ja 5\n",
      "ac 5\n",
      "op 5\n",
      "ls 5\n",
      "ti 5\n",
      "ry 5\n",
      "ai 4\n",
      "rh 4\n",
      "id 4\n",
      "dg 4\n",
      "me 4\n",
      "go 4\n",
      "du 4\n",
      "ot 4\n",
      "ka 4\n",
      "lm 4\n",
      "va 4\n",
      "kn 4\n",
      "gi 4\n",
      "gu 4\n",
      "tu 4\n",
      "av 4\n",
      "sm 4\n",
      "br 4\n",
      "bu 4\n",
      "us 4\n",
      "om 4\n",
      "lk 4\n",
      "cu 4\n",
      "cr 4\n",
      "pl 4\n",
      "wn 4\n",
      "ms 4\n",
      "fa 3\n",
      "rb 3\n",
      "mc 3\n",
      "ty 3\n",
      "mi 3\n",
      "gt 3\n",
      "ud 3\n",
      "oa 3\n",
      "kl 3\n",
      "ox 3\n",
      "tl 3\n",
      "ib 3\n",
      "wh 3\n",
      "ff 3\n",
      "pp 3\n",
      "rg 3\n",
      "wr 3\n",
      "fr 3\n",
      "nc 3\n",
      "sa 3\n",
      "sw 3\n",
      "po 3\n",
      "ob 3\n",
      "oy 3\n",
      "yl 3\n",
      "dm 3\n",
      "ya 3\n",
      "mp 3\n",
      "ye 3\n",
      "mm 3\n",
      "bo 3\n",
      "os 3\n",
      "si 3\n",
      "dw 3\n",
      "ys 3\n",
      "im 3\n",
      "nw 2\n",
      "ju 2\n",
      "hn 2\n",
      "rp 2\n",
      "uf 2\n",
      "ew 2\n",
      "ue 2\n",
      "gs 2\n",
      "ub 2\n",
      "su 2\n",
      "of 2\n",
      "ft 2\n",
      "pr 2\n",
      "yn 2\n",
      "sc 2\n",
      "oh 2\n",
      "dy 2\n",
      "yd 2\n",
      "lf 2\n",
      "fe 2\n",
      "nu 2\n",
      "mb 2\n",
      "nl 2\n",
      "rw 2\n",
      "nr 2\n",
      "ko 2\n",
      "tw 2\n",
      "hl 2\n",
      "nm 2\n",
      "rm 2\n",
      "lu 2\n",
      "lc 2\n",
      "yo 2\n",
      "bl 2\n",
      "rk 2\n",
      "og 2\n",
      "fl 2\n",
      "ip 2\n",
      "gl 2\n",
      "ev 2\n",
      "ba 2\n",
      "wl 2\n",
      "ui 2\n",
      "cl 2\n",
      "sp 2\n",
      "lt 2\n",
      "eb 2\n",
      "ht 2\n",
      "ah 2\n",
      "ca 1\n",
      "ia 1\n",
      "hw 1\n",
      "eh 1\n",
      "rv 1\n",
      "ct 1\n",
      "ep 1\n",
      "mu 1\n",
      "gd 1\n",
      "ci 1\n",
      "sy 1\n",
      "yk 1\n",
      "eg 1\n",
      "nf 1\n",
      "jo 1\n",
      "cc 1\n",
      "pk 1\n",
      "kr 1\n",
      "ez 1\n",
      "wt 1\n",
      "af 1\n",
      "py 1\n",
      "mn 1\n",
      "ix 1\n",
      "xo 1\n",
      "ap 1\n",
      "sf 1\n",
      "lr 1\n",
      "tc 1\n",
      "ok 1\n",
      "mk 1\n",
      "sn 1\n",
      "ov 1\n",
      "km 1\n",
      "tz 1\n",
      "zp 1\n",
      "tm 1\n",
      "ky 1\n",
      "ab 1\n",
      "ef 1\n",
      "vo 1\n",
      "by 1\n",
      "rc 1\n",
      "ce 1\n",
      "pu 1\n",
      "xv 1\n",
      "yr 1\n",
      "uv 1\n",
      "ws 1\n",
      "my 1\n",
      "yt 1\n",
      "ph 1\n",
      "ps 1\n",
      "za 1\n",
      "ao 1\n",
      "if 1\n",
      "rf 1\n",
      "eo 1\n",
      "uh 1\n",
      "fu 1\n",
      "sd 1\n",
      "xa 1\n",
      "ts 1\n"
     ]
    }
   ],
   "source": [
    "# sort freqencies\n",
    "freq_sorted(eng_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_gram = generate_bigrams(russian_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ov 342\n",
      "in 198\n",
      "ko 159\n",
      "ev 133\n",
      "ch 117\n",
      "an 111\n",
      "sk 107\n",
      "ha 101\n",
      "ky 94\n",
      "ba 92\n",
      "he 81\n",
      "er 79\n",
      "en 77\n",
      "ik 76\n",
      "kh 76\n",
      "ak 76\n",
      "ro 75\n",
      "hi 74\n",
      "no 71\n",
      "zh 68\n",
      "li 66\n",
      "nk 65\n",
      "sh 65\n",
      "al 65\n",
      "ar 64\n",
      "ki 62\n",
      "ts 60\n",
      "ya 58\n",
      "vi 56\n",
      "lo 55\n",
      "vs 55\n",
      "le 53\n",
      "ma 53\n",
      "ho 51\n",
      "to 50\n",
      "el 46\n",
      "re 40\n",
      "va 39\n",
      "ag 39\n",
      "la 39\n",
      "il 38\n",
      "on 38\n",
      "ai 37\n",
      "ja 37\n",
      "ni 37\n",
      "ab 36\n",
      "as 36\n",
      "ra 36\n",
      "uk 36\n",
      "ka 35\n",
      "se 33\n",
      "ri 33\n",
      "is 33\n",
      "mo 32\n",
      "po 32\n",
      "ve 32\n",
      "mi 31\n",
      "ol 31\n",
      "ti 31\n",
      "ic 30\n",
      "ad 30\n",
      "be 30\n",
      "at 30\n",
      "or 30\n",
      "ul 29\n",
      "im 28\n",
      "av 28\n",
      "ga 28\n",
      "ae 27\n",
      "di 26\n",
      "it 26\n",
      "hu 25\n",
      "ah 25\n",
      "us 24\n",
      "nt 24\n",
      "tu 24\n",
      "ir 24\n",
      "am 23\n",
      "gu 23\n",
      "go 22\n",
      "mu 22\n",
      "ns 22\n",
      "yu 22\n",
      "za 22\n",
      "so 21\n",
      "ur 21\n",
      "da 21\n",
      "ub 21\n",
      "ne 21\n",
      "de 21\n",
      "az 21\n",
      "ly 21\n",
      "ot 21\n",
      "ru 21\n",
      "do 20\n",
      "rt 20\n",
      "ta 20\n",
      "gr 20\n",
      "tz 20\n",
      "ut 20\n",
      "un 19\n",
      "et 19\n",
      "st 19\n",
      "ji 19\n",
      "ok 18\n",
      "uh 18\n",
      "ny 18\n",
      "rk 18\n",
      "ih 17\n",
      "gi 17\n",
      "ff 17\n",
      "sc 17\n",
      "bi 16\n",
      "ek 16\n",
      "ht 16\n",
      "hk 16\n",
      "ge 15\n",
      "bo 15\n",
      "pa 15\n",
      "je 15\n",
      "du 15\n",
      "sa 15\n",
      "ib 15\n",
      "of 15\n",
      "os 15\n",
      "lu 15\n",
      "dz 15\n",
      "nd 14\n",
      "ie 14\n",
      "iv 14\n",
      "ei 14\n",
      "pi 14\n",
      "nu 13\n",
      "ud 13\n",
      "br 13\n",
      "hl 13\n",
      "tc 13\n",
      "hm 13\n",
      "me 13\n",
      "te 13\n",
      "aw 13\n",
      "ku 13\n",
      "hn 13\n",
      "es 12\n",
      "si 12\n",
      "eb 12\n",
      "id 12\n",
      "zo 12\n",
      "rs 12\n",
      "ze 12\n",
      "ju 11\n",
      "em 11\n",
      "bl 11\n",
      "na 11\n",
      "ry 11\n",
      "uz 11\n",
      "fi 11\n",
      "og 10\n",
      "ob 10\n",
      "dr 10\n",
      "gl 10\n",
      "zi 10\n",
      "ug 10\n",
      "om 10\n",
      "ld 10\n",
      "vy 9\n",
      "nc 9\n",
      "ke 9\n",
      "ys 9\n",
      "od 9\n",
      "dj 9\n",
      "my 8\n",
      "nn 8\n",
      "oh 8\n",
      "au 8\n",
      "lk 8\n",
      "bu 8\n",
      "pe 8\n",
      "ez 8\n",
      "ap 8\n",
      "vl 7\n",
      "vo 7\n",
      "ls 7\n",
      "ig 7\n",
      "su 7\n",
      "ed 7\n",
      "sy 7\n",
      "ee 7\n",
      "ng 7\n",
      "rn 7\n",
      "tr 7\n",
      "yk 7\n",
      "dy 7\n",
      "yr 7\n",
      "ep 7\n",
      "rc 7\n",
      "ks 7\n",
      "oc 6\n",
      "pr 6\n",
      "tn 6\n",
      "zy 6\n",
      "ty 6\n",
      "fa 6\n",
      "ac 6\n",
      "hv 6\n",
      "yz 6\n",
      "if 5\n",
      "uj 5\n",
      "oz 5\n",
      "up 5\n",
      "rm 5\n",
      "jo 5\n",
      "af 5\n",
      "iy 5\n",
      "op 5\n",
      "rd 5\n",
      "rg 5\n",
      "aj 5\n",
      "eh 5\n",
      "lt 5\n",
      "we 5\n",
      "rb 4\n",
      "by 4\n",
      "mc 4\n",
      "gm 4\n",
      "ij 4\n",
      "oj 4\n",
      "um 4\n",
      "zk 4\n",
      "ll 4\n",
      "dn 4\n",
      "fe 4\n",
      "bk 4\n",
      "td 4\n",
      "iz 4\n",
      "yc 4\n",
      "kl 4\n",
      "ej 4\n",
      "lm 4\n",
      "yh 4\n",
      "dk 4\n",
      "o  4\n",
      " t 4\n",
      "th 4\n",
      "e  4\n",
      " f 4\n",
      "t  4\n",
      " p 4\n",
      "kr 3\n",
      "yl 3\n",
      "rh 3\n",
      "oe 3\n",
      "vk 3\n",
      "lb 3\n",
      "yj 3\n",
      "jk 3\n",
      "ln 3\n",
      "eg 3\n",
      "ow 3\n",
      "ws 3\n",
      "ms 3\n",
      "vr 3\n",
      "gn 3\n",
      "ay 3\n",
      "sl 3\n",
      "lg 3\n",
      "ef 3\n",
      "tk 3\n",
      "wa 3\n",
      "tm 3\n",
      "dl 3\n",
      "ia 3\n",
      "nz 3\n",
      "sn 3\n",
      "vn 3\n",
      "oi 3\n",
      "ou 2\n",
      "bd 2\n",
      "jm 2\n",
      "zd 2\n",
      "vd 2\n",
      "vt 2\n",
      "yg 2\n",
      "ue 2\n",
      "rz 2\n",
      "hc 2\n",
      "sf 2\n",
      "lc 2\n",
      "hs 2\n",
      "pk 2\n",
      "dc 2\n",
      "wi 2\n",
      "mb 2\n",
      "wr 2\n",
      "zu 2\n",
      "mp 2\n",
      "jb 2\n",
      "sv 2\n",
      "bs 2\n",
      "zn 2\n",
      "py 2\n",
      "km 2\n",
      "l' 2\n",
      "io 2\n",
      "fr 2\n",
      "pp 2\n",
      "hb 2\n",
      "ss 2\n",
      "rj 2\n",
      "ip 2\n",
      "fu 2\n",
      "dt 2\n",
      "lj 1\n",
      "rp 1\n",
      "fk 1\n",
      "ui 1\n",
      "np 1\n",
      "sm 1\n",
      "iu 1\n",
      "wt 1\n",
      "sp 1\n",
      "bb 1\n",
      "ce 1\n",
      "oy 1\n",
      "gd 1\n",
      "fo 1\n",
      "uc 1\n",
      "ft 1\n",
      "tl 1\n",
      "ym 1\n",
      "jv 1\n",
      "gy 1\n",
      "bt 1\n",
      "hw 1\n",
      "zr 1\n",
      "ea 1\n",
      "ec 1\n",
      "pl 1\n",
      "mg 1\n",
      "lz 1\n",
      "jd 1\n",
      "lv 1\n",
      "zv 1\n",
      "yb 1\n",
      "rl 1\n",
      "jy 1\n",
      "lp 1\n",
      "yp 1\n",
      "mn 1\n",
      "nj 1\n",
      "'k 1\n",
      "eu 1\n",
      "hd 1\n",
      "hg 1\n",
      "pt 1\n",
      "ey 1\n",
      "jj 1\n",
      "nr 1\n",
      "kt 1\n",
      "uf 1\n",
      "bn 1\n",
      "wd 1\n",
      "n- 1\n",
      "-k 1\n",
      "y- 1\n",
      "-s 1\n",
      "ml 1\n",
      "zs 1\n",
      "vg 1\n",
      "nb 1\n",
      "vz 1\n",
      "jn 1\n",
      "zg 1\n",
      "gv 1\n",
      "nf 1\n",
      "bc 1\n",
      "hr 1\n",
      "mj 1\n",
      "jg 1\n",
      "zm 1\n",
      "'n 1\n",
      "co 1\n",
      "jl 1\n",
      "ds 1\n",
      "mt 1\n"
     ]
    }
   ],
   "source": [
    "freq_sorted(rus_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: What bigram is most informative for distinguishing between English and Russian names?\n",
    "\n",
    "__Obervation__: English top 5 bigrams:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "er : 49\n",
    "\n",
    "on : 47\n",
    "\n",
    "ar : 40\n",
    "\n",
    "in : 37\n",
    "\n",
    "le : 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: Russian top 5 bigrams:\n",
    "\n",
    "ov : 342\n",
    "\n",
    "in : 198\n",
    "\n",
    "ko : 159\n",
    "\n",
    "ev : 133\n",
    "\n",
    "ch : 117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = surname_df[\"surname\"] # features (x) needed to predict nationatlity\n",
    "target = surname_df[\"nationality\"] # what we are predicting (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Naiive Bayes\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels for english only\n",
    "# surname_df['label'] = [1 if x =='English' else 0 for x in surname_df['nationality']]\n",
    "# english_labels = surname_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surname_df.to_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surname_df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: There are significantly more russian names in the dataset than english. The data is not quite balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize features\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Data\n",
    "\n",
    "To make the data a little more accurate in it's predictions, we are going to split the surnames into train (65%) and test (35%) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 1306]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a7025cf24d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split the data to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 256\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 1306]"
     ]
    }
   ],
   "source": [
    "# split the data to train the model\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.35, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848, 1308)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-cd834245ad61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msurname_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msurname_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "surname_model = MultinomialNB()\n",
    "surname_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = surname_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_test_csv = \"data_set/surnames_test.csv\"\n",
    "surname_test = pd.read_csv(surname_test_csv, index_col = None, encoding=\"UTF-8\")\n",
    "surname_test.rename(columns = {'Unnamed: 0':'surname', 'Unnamed: 1':'nationality'}, inplace = True)\n",
    "\n",
    "surname_test_list = surname_test['surname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_test_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_test['label'] = [1 if x =='English' else 0 for x in surname_test['nationality']]\n",
    "labels = surname_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "cv_feature = cv.transform(surname_test_list)\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(cv_feature)\n",
    "reshape_feature = tf_transformer.transform(cv_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishness = n_model.predict(reshape_feature)\n",
    "englishness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# russianess = n_model.predict(reshape_feature)\n",
    "# russianess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -Model Summary-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to same type as russianess (y_pred)\n",
    "reshape_feature = reshape_feature.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "OLS(labels,russianess).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -Observations-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_name1 = [\"Wasem\"]\n",
    "reshape_feature = cv.transform(pred_name1)\n",
    "russian_model.predict(reshape_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: __Wasem__ is an Arabic name. Model seems to think it is Russian due to similarity is spelling. Misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_name2 = [\"See\"]\n",
    "reshape_feature = cv.transform(pred_name2)\n",
    "russian_model.predict(reshape_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: __See__ is a dutch name. This is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_name3 = [\"Los\"]\n",
    "reshape_feature = cv.transform(pred_name3)\n",
    "russian_model.predict(reshape_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: __Los__ is a Russian name. This has been misclassified as not Russian most likely due to Spanish having a similar name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
